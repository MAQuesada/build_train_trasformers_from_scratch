{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Análisis de la complejidad temporal del trasnformer:\n",
    "\n",
    "Analizar la complejidad temporal de un modelo de transformer completo es todo un  desafio debido a la interacción de múltiples componentes complejos, incluyendo capas de Attention múltiple, conexiones residuales, normalización de capas y redes de alimentación hacia adelante. Cada uno de estos componentes contribuye de manera significativa al comportamiento y rendimiento del modelo, pero su interacción complica la estimación precisa de la complejidad temporal global. Por esta razón, se opta por centrarse en el análisis de la complejidad del mecanismo de Attention, que es el bloque de construcción fundamental de los transformers. \n",
    "\n",
    "El mecanismo de Attention se muestra en la siguiente figura:\n",
    " \n",
    "![image.png](../resources/Neural%20network%20representation%20of%20Attention.webp)\n",
    "\n",
    "Como se puede apreciar intervienen tres matrices fundamentales **QUERY (Q),KEY (K) Y VALUE (V)**, siempre la matriz K y V coinciden y es porque el objetivo de un bloque de Attention es calcular la similitud existente entre Q y V. Entonces cuando hablamos de **Multi-Head Attention**  no refererimos a ejecutar el mismo procedimiento sobre Q,K,V pero utilizando en lugar de una sola matriz de pesos, usando una por cada head, donde la Attention se calcula de manera que cada head aprende a centrarse en diferentes partes de la entrada. Este comportamiento se evidencia en la siguiente figura que muestra como queda el bloque de Attention para 3 heads.\n",
    " \n",
    "![image.png](../resources/multi_head_attention_block.png)\n",
    " \n",
    "\n",
    "\n",
    "Este mecanismo se puede realizar de manera paralella por cada head, por lo que no aumenta la complejidad temporal, solo se aumenta la complejidad temporal en la concatenación de los resultados de cada head en uno solo.  Por lo tanto  la complejidad temporal se descompone en dos partes principales: el cálculo de la Attention y la concatenación de los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Cálculo de la Attention**: Para una sola head de Attention, se calculan tres conjuntos de pesos ( query( $Q$ )$, key$( $K$ ) , y value ( $V$ ) ) a partir de la entrada. Considerando que la dimensión de la entrada (longitud de la secuencia) es  $n$  y $d$  es dimensión de la representación se tiene:\n",
    "\n",
    "   * Para calcular cada uno de los pesos $Q, K y V$ es necesario realizar una multiplicacion de la matriz de entrada con dimensiones  $ (n  \\times d ) $ por la matriz de la capa linear, de dimensión $( d \\times d )$, obteniendo una complejidad en este paso de  $( O(n \\cdot d^2) )$. Finalmente estos pesos se dividen en matrices de dimensiones $ (n  \\times d_k ) $, donde $d_k$ es la dimensión de cada head de Attention y típicamente $( d_k = d/h )$ con $h $ siendo el número de heads de Attention. \n",
    "\n",
    "   * La Attention se calcula usando un producto punto escalado entre Q y K, seguido por una función softmax para obtener los $h_i-Weights$ de Attention, y finalmente se multiplican estos pesos por V. Esto implica una multiplicación de matrices de $( n \\times d_k )$ con $( d_k \\times n )$, resultando en una matriz de $( n \\times n )$, para cada head de Attention. Por lo tanto, la complejidad de este paso es $( O(n^2 \\cdot d_k) )$.\n",
    "\n",
    "2. **Concatenación de los resultados**: Después de calcular la Attention para cada head, los resultados de todas las heads se concatenan y se multiplican por una matriz de concatenación final de dimensión $( d \\times d )$. Por lo tanto, la complejidad de este paso es $( O(n \\cdot d^2) )$.\n",
    "\n",
    "Dado que estas operaciones se realizan para cada una de las $( h )$ heads en paralelo, la complejidad total de la capa de Attention multi-head, considerando $( h )$ heads, es:\n",
    "$[ O(  n \\cdot d^2 + h \\cdot n^2 \\cdot d_k + n \\cdot d^2) ]$. \n",
    " \n",
    "Sin embargo, como $( d_k = d/h )$  y $O( n \\cdot d^2 + n \\cdot d^2)  = O( n \\cdot d^2)$. , la complejidad se puede simplificar y reescribir como:\n",
    " \n",
    "$ O(n^2 \\cdot d + n \\cdot d^2) $\n",
    "\n",
    "Esto muestra que la complejidad depende cuadráticamente de la longitud de la secuencia $( n )$ y de la dimensión de la representación $( d )$. Esto es importante para entender el rendimiento y las limitaciones computacionales de los modelos basados en Transformer, especialmente en contextos donde $( n )$ o $( d )$ son grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
