# Transformers, a deep analysis 

In this repository we create a transformer from scratch based on the "attention is all you need" paper. We go step by step through the construction of the model. Finally we compare its accuracy with an LSTM model in the German to English translation task. In addiction, we show an under-the-hood analysis of the attention mechanism.